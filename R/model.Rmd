```{r}
library(tidyverse)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(here)
library(vip)
library(doParallel)

registerDoParallel(cores = 3)
```

```{r}
df <- read_csv(here("data", "clean.csv")) %>%
  mutate(
    description = ifelse(is.na(description), "MISSING", description)
  )
```

# Modeling

We will try to construct a model that predicts description to a language using tf-idf vectors.  We will use three models:

* k-NN model with cosine similarity
* LASSO
* XGBoost

```{r}
set.seed(54321)

split <- initial_split(df, strata = "language", prop = 0.8)
train <- training(split)
test <- testing(split)
folds <- vfold_cv(train, v = 5)

# save indices for python analysis
split %>%
  tidy() %>%
  mutate(Row = Row - 1) %>%
  write_csv(here("data", "indices.csv"))

mset <- metric_set(mn_log_loss, accuracy)
control <- control_grid(
  save_workflow = TRUE,
  save_pred = TRUE,
  verbose = TRUE
)
```

Here is a master recipe where we

1. strip non-ascii words and excess whitespaces
2. filter out repos with too few words
3. tokenize description and filter out stopwords (including language names)
4. add tf-idf

```{r}
master_rec <- recipe(language ~ description, data = df) %>%
  step_mutate(
    description = str_replace_all(description, "[^\\x00-\\x7F]", " "),
    description = str_replace_all(description, "\\s+", " "),
    nwords = str_count(description, "\\w+")
  ) %>%
  step_filter(nwords >= 2, skip = TRUE) %>%
  step_rm(nwords) %>%
  step_tokenize(description) %>%
  step_stopwords(description) %>%
  step_stopwords(description, custom_stopword_source = c("r", "java", "javascript", "js", "python", "c")) %>%
  step_tokenfilter(description, max_tokens = tune()) %>%
  step_tfidf(description)
```

### Model and workflow for k-NN

```{r}
knn_model <- nearest_neighbor(
  mode = "classification",
  neighbors = tune(),
  weight_func = "cos"
) %>%
  set_engine("kknn")

knn_workflow <- workflow() %>%
  add_recipe(master_rec) %>%
  add_model(knn_model)
```

We will tune on number of neighbors and max number of tokens.

```{r}
knn_path <- here("outputs", "knn_res.rds")
override <- FALSE

if (!file.exists(knn_path) || override) {
  knn_res <- knn_workflow %>%
    tune_grid(folds,
      metrics = mset,
      control = control,
      grid = crossing(
        neighbors = floor(seq(50, 250, length.out = 5)),
        max_tokens = floor(seq(50, 350, length.out = 5)),
      )
    )

  saveRDS(knn_res, knn_path)
}
```

```{r}
knn_res <- readRDS(knn_path)

knn_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  arrange(-mean)

autoplot(knn_res)
```

* Best: 0.472
* neighbors: 250
* max_tokens: 350

0.472 accuracy on a 5-class classifier isn't too bad.

Now evaluate on test set.

```{r}
final_model <- knn_workflow %>%
  finalize_workflow(select_best(knn_res, metric = "accuracy")) %>%
  fit(train)

preds <- final_model %>%
  augment(test)

preds %>%
  with(accuracy_vec(as.factor(language), .pred_class))
```

Let's look at the accuracy for each language.

```{r}
preds %>%
  mutate(acc = .pred_class == language) %>%
  group_by(language) %>%
  summarize(mean(acc))
```

Python is the worst performing.  Makes sense given how Python is the second best language at everything.

### LASSO model and workflow

```{r}
lin_rec <- master_rec %>%
  step_normalize(all_numeric_predictors())

lin_model <- multinom_reg(
  mode = "classification",
  penalty = tune()
) %>%
  set_engine("glmnet")

lin_workflow <- workflow() %>%
  add_recipe(lin_rec) %>%
  add_model(lin_model)
```

We will tune over beta penalty and number of tokens.  We can do more tokens than knn because LASSO can handle sparse data.

```{r}
lin_path <- here("outputs", "lin_res.rds")
override <- FALSE

if (!file.exists(lin_path) || override) {
  lin_res <- lin_workflow %>%
    tune_grid(folds,
      metrics = mset,
      control = control,
      grid = crossing(
        penalty = 10^seq(-7, -1, 0.2),
        max_tokens = floor(seq(200, 1000, length.out = 5))
      )
    )

  saveRDS(lin_res, lin_path)
}
```

```{r}
lin_res <- readRDS(lin_path)

lin_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  arrange(-mean)

autoplot(lin_res)
```

* best: 0.601
* max_tokens: 1000

Now evaluate on test set.

```{r}
final_model <- lin_workflow %>%
  finalize_workflow(select_best(lin_res, metric = "accuracy"))

fitted_model <- final_model %>%
  fit(train)

preds <- fitted_model %>%
  augment(test)

preds %>%
  with(accuracy_vec(as.factor(language), .pred_class))
```

LASSO continues to outperform.

Let's look at how LASSO performs by language.

```{r}
preds %>%
  mutate(acc = .pred_class == language) %>%
  group_by(language) %>%
  summarize(mean(acc))
```

### XGBoost model and workflow

```{r}
xgb_model <- boost_tree(
  mode = "classification",
  learn_rate = tune(),
  trees = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost")

xgb_workflow <- workflow() %>%
  add_recipe(master_rec) %>%
  add_model(xgb_model)
```

We will tune on number of trees, the learning rate, and number of columns sampled at each node.

```{r}
xgb_path <- here("outputs", "xgb_res.rds")
override <- FALSE

if (!file.exists(xgb_path) || override) {
  xgb_res <- xgb_workflow %>%
    tune_grid(folds,
      metrics = mset,
      control = control,
      grid = crossing(
        trees = c(200, 400, 600, 800, 1000),
        learn_rate = 10^seq(-3, -1, length.out = 5),
        mtry = c(10, 30, 100),
        max_tokens = floor(seq(250, 1000, length.out = 4))
      )
    )

  saveRDS(xgb_res, xgb_path)
}
```

```{r}
xgb_res <- readRDS(xgb_path)

xgb_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  arrange(-mean)
```

* best: 0.608
* learn_rate: 0.1
* mtry: 100
* trees: 1000
* max_tokens: 1000

The autoplot is jumbled, so we manually plot.

```{r}
xgb_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(learn_rate = as.factor(round(learn_rate, digits = 3))) %>%
  ggplot(aes(x = trees, y = mean, color = learn_rate)) +
  geom_line() +
  geom_point() +
  facet_grid(rows = vars(max_tokens), cols = vars(mtry))
```

Looks like increasing max_tokens could help a little more.  Increasing mtry seems to improve only the small learning rates models, but probably won't give a large improvement overall.

Now evaluate on test set.

```{r}
final_model <- xgb_workflow %>%
  finalize_workflow(select_best(xgb_res, metric = "accuracy"))

fitted_model <- final_model %>%
  fit(train)

preds <- fitted_model %>%
  augment(test)

preds %>%
  with(accuracy_vec(as.factor(language), .pred_class))
```

This is an incremental improvement over LASSO and is pretty far from the transformer model we will use in the end so we will not further optimize over parameters.

Let's look at the learning curves.  We will subsample sets and evaluate the model.

```{r}
n <- 4
p <- (1:n) / n
grid <- expand.grid(1:nrow(folds), p) %>%
  as_tibble() %>%
  rename(fold = Var1, p = Var2)

grid_path <- here("outputs", "grid.rds")
override <- FALSE

if (!file.exists(grid_path) || override) {
  err_tot <- foreach(i = 1:nrow(grid)) %dopar% {
    train_df <- folds$splits[[grid$fold[i]]] %>%
      analysis() %>%
      slice_sample(prop = grid$p[i])

    fitted <- final_model %>%
      fit(train_df)

    error_train <- fitted %>%
      augment(train_df) %>%
      with(accuracy_vec(as.factor(language), .pred_class))

    error_val <- fitted %>%
      augment(assessment(folds$splits[[grid$fold[i]]])) %>%
      with(accuracy_vec(as.factor(language), .pred_class))

    c(error_train, error_val)
  }

  res <- grid %>%
    mutate(
      train = map_dbl(err_tot, ~ .[1]),
      val = map_dbl(err_tot, ~ .[2])
    )

  saveRDS(res, grid_path)
}
```

```{r}
res <- readRDS(grid_path)

res %>%
  group_by(p) %>%
  summarize(
    train = mean(train),
    val = mean(val)
  ) %>% 
  pivot_longer(train:val, names_to = "type", values_to = "score") %>%
  ggplot(aes(x = p)) +
  geom_line(aes(y = score, color = type)) +
  geom_point(aes(y = score, color = type)) +
  ylim(0, 1)
```

The curves look like they've asymptoted nicely so gathering more data probably would only make marginal improvements.  There is a decent gap, suggesting overfitting.

Finally, let's look at feature importance.

```{r}
fitted_model %>%
  extract_fit_engine() %>%
  vip(n = 25)
```

List of highly important terms.  These terms look pretty predictive of the language (android, react, pytorch, etc.).

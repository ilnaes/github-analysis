```{r}
library(tidyverse)
library(tidymodels)
library(tidytext)
library(textrecipes)
```

```{r}
df <- read_csv("data/clean.csv") |>
  mutate(
    description = ifelse(is.na(description), "MISSING", description),
    stars = log(stars)
  )
```

We now do a more in-depth analysis of repo descriptions.

```{r}
word_count <- df |>
  mutate(nwords = str_count(description, "\\w+"))

word_count |>
  ggplot(aes(nwords)) +
  scale_x_log10() +
  geom_histogram()
```

Word count is very skewed.

```{r}
word_count |>
  arrange(-nwords) |>
  head(10) |>
  pluck("description")
```

It looks like many wordy descriptions contain Chinese (with some mixed in English usually).  Since we want to do an English-based analysis, we will strip out all non-English words.

```{r}
df <- df |>
  mutate(
    description = str_replace_all(description, "[^A-Za-z0-9\\s'-\\.]", " "),
    description = str_replace_all(description, "\\s+", " "),
    nwords = str_count(description, "\\w+")
  )

word_count <- df |>
  mutate(nwords = str_count(description, "\\w+"))
```

Let's see if word count has any correlation with stars.

```{r}
word_count |>
  ggplot(aes(nwords, stars)) +
  scale_x_log10() +
  geom_point() +
  facet_wrap(vars(language), scales = "free")
```

Not really.

## Word analysis

We first tokenize words.  Let's look at most popular words.

```{r}
word_list <- word_count |>
  unnest_tokens(word, description) |>
  anti_join(stop_words)

word_list |>
  count(word) |>
  arrange(-n)
```

Let's see if this changes by language.  Let's just look at top 5

```{r}
word_list |>
  count(word, language) |>
  group_by(language) |>
  slice_max(n, n = 5) |>
  ggplot(aes(n, word)) +
  geom_col() +
  facet_wrap(vars(language), scales = "free")
```

Nothing surprising here.  Note that the pairs (js, ts) and (py, ipynb) are pretty similar, as to be expected.

# Clustering

Given how nicely topics seem to appear from the most popular words, let's try some correlation and clustering.

Let's first see how often pairs of words appear together in a description.

```{r}
pairs <- word_count |>
  filter(nwords >= 2) |>
  select(full_name, language, description) |>
  mutate(d1 = description) |>
  unnest_tokens(word1, description) |>
  anti_join(stop_words, by = c("word1" = "word")) |>
  distinct(full_name, language, d1, word1) |>
  unnest_tokens(word2, d1) |>
  anti_join(stop_words, by = c("word2" = "word")) |>
  distinct(full_name, language, word1, word2) |>
  filter(word1 < word2) |>
  select(-full_name)

pairs |>
  count(word1, word2) |>
  arrange(-n)
```

Let's now look at bigrams by language.

```{r}
pairs |>
  unite("bigram", word1:word2) |>
  group_by(language) |>
  count(bigram) |>
  slice_max(n, n = 5) |>
  ggplot(aes(n, bigram)) +
  geom_col() +
  facet_wrap(vars(language), scales = "free")
```

Let's look at a correlation graph.  We will throw out "library" because that's basically a stop word on Github.  We then assign each pair to the language that uses it the most.

```{r}
pairs_count <- pairs |>
  filter(word1 != "library", word2 != "library") |>
  group_by(language, word1, word2) |>
  summarize(lang_count = n()) |>
  group_by(word1, word2) |>
  mutate(n = sum(lang_count)) |>
  filter(n >= 5) |>
  slice_max(lang_count, n = 1) |>
  ungroup() |>
  select(-lang_count)
```

We now plot the pairs graph.  We throw out some languages to make the colors more differentiable. 23 was chosen by tuning.

```{r}
library(igraph)
library(ggraph)

pairs_count |>
  filter(
    n >= 23,
    language %in% c("R", "Python", "JavaScript", "Go", "C", "C++", "Java")
  ) |>
  relocate(language, .after = word2) |>
  graph_from_data_frame() |>
  ggraph(layout = "fr") +
  geom_edge_link(aes(color = language)) +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

# Visualizations

```{r}
cos_rec <- recipe(language ~ description + nwords, data = word_count) |>
  step_filter(nwords >= 2, skip = TRUE) |>
  step_rm(nwords) |>
  step_tokenize(description) |>
  step_stopwords(description) |>
  step_tokenfilter(description, max_tokens = 300) |>
  step_tfidf(description)
```

```{r}
prepped_df <- cos_rec |>
  prep() |>
  juice() |>
  add_count(across(where(is.numeric))) |>
  filter(n == 1)
```

PCA plot

```{r}
pca_df <- prepped_df |>
  select(-language, -n) |>
  prcomp(scale = TRUE, center = TRUE)

pca_df$x |>
  as_tibble() |>
  bind_cols(select(prepped_df, language)) |>
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(color = language), alpha = 0.5)
```

t-SNE plot

```{r}
library(Rtsne)

tsne <- prepped_df |>
  select(-language, -n) |>
  Rtsne()

tsne$Y |>
  as_tibble() |>
  bind_cols(prepped_df |> select(language)) |>
  ggplot(aes(V1, V2)) +
  geom_point(aes(color = language), alpha = 0.5)
```

UMAP plot

```{r}
library(umap)

umap_df <- prepped_df |>
  select(-language, -n) |>
  umap()

umap_df$layout |>
  as_tibble() |>
  bind_cols(select(prepped_df, language)) |>
  ggplot(aes(V1, V2)) +
  geom_point(aes(color = language), alpha = 0.5) +
  xlim(-5, 4) +
  ylim(-5, 4)
```

# Modelling

We will try to construct a model that predicts which matches a description to a language.

We will use two methods:

1. Cosine similarity on tfidf vectors
2. A simple n-gram model

```{r}
cos_rec <- recipe(language ~ description + nwords, data = word_count) |>
  step_filter(nwords >= 2, skip = TRUE) |>
  step_rm(nwords) |>
  step_tokenize(description) |>
  step_stopwords(description) |>
  step_tokenfilter(description, max_tokens = tune()) |>
  step_tfidf(description)

cos_model <- nearest_neighbor(mode = "classification",
                              neighbors = tune(),
                              weight_func = "cos") |>
  set_engine("kknn")

cos_workflow <- workflow() |>
  add_recipe(cos_rec) |>
  add_model(cos_model)
```

```{r}
set.seed(54321)

split <- initial_split(word_count, strata = "language", prop = 0.8)
train <- training(split)
test <- testing(split)
folds <- vfold_cv(train, v = 5)

mset <- metric_set(mn_log_loss, accuracy)
control <- control_grid(save_workflow = TRUE,
                        # save_pred = TRUE,
                        # extract = extract_model,
                        verbose = TRUE)
```

```{r}
res <- cos_workflow |> 
  tune_grid(folds,
            metrics = mset,
            control = control,
            grid = crossing(neighbors = c(28),
                            max_tokens = c(150)),
            )

# save(res, file = "res.RData")
```

Parameters come from tuning

```{r}
# load("res.RData")
autoplot(res)
```

```{r}
final_model <- cos_workflow |> 
  finalize_workflow(select_best(res, metric = "accuracy")) |> 
  fit(train)

preds <- final_model |> 
  augment(test)

preds |> 
  mutate(acc = .pred_class == language) |> 
  group_by(language) |> 
  summarize(mean(acc))
```
